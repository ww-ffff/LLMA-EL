Entity Linking (EL) is a key technology in NLP downstream applications such as reading comprehension and intelligent question answering. However, EL faces two major challenges: Firstly, the generation of candidate sets usually relies on small models, which often fail to produce high-quality candidate sets due to a lack of deep semantic understanding, resulting in insufficient precision and coverage. Secondly, the problem of the absence of correct entities in the candidate sets is often not addressed by existing research, severely impacting its robustness in practical applications. To address these issues, we propose the Large Language Model augmented entity linking framework (LLMA-EL). LLMA-EL improves the quality of candidate sets through tool adaptation techniques and novel enhanced semantic understanding prompt, ensuring the precision of candidates. Additionally, we have designed a candidate adaptive iteration strategy and multi-choice prompt that optimizes candidate sets lacking correct entities, and transforms the EL task into a multi-choice task, enhancing the adaptability and accuracy of large language models in performing linking tasks. Experimental results show that LLMA-EL achieves state-of-the-art performance on four classic datasets. 
